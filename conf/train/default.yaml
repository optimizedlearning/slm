max_steps: 1000
max_time_hours: null # train for at most this many hours
val_check_interval: 400
val_batches: 200
precision: amp_fp16 # for mixed precision in composer: amp_fp16. In pytorch lighting: 16-mixed
gradient_clip_val: 1.0
gradient_clip_algorithm: norm
lr_warmup: 1000
lr_decay: linear
lr: 0.0001
weight_decay: 0.01
per_device_batch_size: 64 # number of examples placed on each GPU
compile: True

log_bits_per_byte: True


# following settings chosen after
# some experimentation with a tiny model.
# may not be optimal for all machines, but
# hopefully with a reasonably sized model this will
# prevent dataloading from being the bottleneck.
dataloader_workers: 2
tokenizer_batch_size: 1 # this is the batch size for parellelism in the huggingface dataset, not the standard batch size
prefetch_factor: null

